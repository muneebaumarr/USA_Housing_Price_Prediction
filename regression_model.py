# -*- coding: utf-8 -*-
"""Regression Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12ifuUIsct6kYkxjUzv8XVKzqEQMQW8Q3
"""

# Commented out IPython magic to ensure Python compatibility.
#importing Libraries

import numpy as np
import pandas as pd
import seaborn as sns
import sklearn
import matplotlib.pyplot as plt
# %matplotlib inline
import warnings
warnings.filterwarnings('ignore')
plt.rcParams['figure.figsize']=[10, 5]
warnings.simplefilter(action='ignore', category=FutureWarning)

df = pd.read_csv('/content/USA_Housing.csv')
df.head()

df.shape

df.info()

df.isnull().sum()

df.drop('Address', axis=1 , inplace=True)

df.dropna(inplace=True)

df

#numeric summary
df.describe()

"""# **Objective: Machine Learning**

Next, I will feed these features into various classification algorithms to determine the best performance using a simple frame work: split, fir. Prdict and Score.

**Target Variable Splitting**

We will split the full dataset into Input And Target Variables.
"""

#splitting Data

x = df.drop('Price', axis=1)

y= df['Price'] #We will predict 'y'

from sklearn import preprocessing
pre_processing= preprocessing.StandardScaler().fit(x)
x_transform = pre_processing.fit_transform(x)

x_transform.shape
x_transform

y.shape

from sklearn.model_selection import train_test_split
x_train, x_test , y_train, y_test = train_test_split(x_transform , y , test_size =.10, random_state=101)

#model training

from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(x_train, y_train)

#Prediction
y_pred = lin_reg.predict(x_test)
y_pred.shape
#y_pred

#Combining actual values and predicited values
results = np.column_stack((y_test, y_pred))
print("Actual Values     |   Predicited Values")
print("---"*40)
for actual , predicted in results:
  print(f"{actual:14.2f}    |   {predicted:14.2f}")

"""# **Residual Analysis**"""

residual = actual - y_pred.reshape(-1)
print(residual)

sns.distplot(residual, kde=True)

#Mean Square Error

from sklearn.metrics import mean_squared_error
print("Linear Regression")
print("---"*30)

mse= mean_squared_error(y_test , y_pred)
rmse = np.sqrt(mse)
print(f"Mean Square Error:{mse}")
print(f"Root Mean Square Error:{rmse}")

"""# **Decision Tree**"""

from sklearn.tree import DecisionTreeRegressor
dt_reg = DecisionTreeRegressor()
dt_reg.fit(x_train , y_train)

#prediciting The sales Price
y_pred_dt = dt_reg.predict(x_test)

Dtr = mean_squared_error(y_pred_dt, y_test)
print("Decision Tree" , Dtr)

"""# **Random Forest**"""

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor

rf_reg = RandomForestRegressor()
rf_reg.fit(x_train , y_train)

#prediciting The sales Price
y_pred_rf = rf_reg.predict(x_test)

rfr = mean_squared_error(y_pred_rf, y_test)
print("Random Forest" , rfr)

"""# **Gradient Boosting Regression**"""

from sklearn.ensemble import GradientBoostingRegressor

gb_reg = GradientBoostingRegressor()
gb_reg.fit(x_train , y_train)

#prediciting The sales Price
y_pred_gb = gb_reg.predict(x_test)

gbr = mean_squared_error(y_pred_gb, y_test)
print("Decision Tree" , gbr)

#Model Scores Comparision

print("Model Ranking (Lower Are Better)")
print("---"*30)

model_scores = {
    "Linear Regression": lin_reg.score(x_test, y_test  ),
    "Decision Tree": dt_reg.score(x_test, y_test),
    "Random Forest": rf_reg.score(x_test, y_test),
    "Gradient Boosting": gb_reg.score(x_test, y_test)
}

model_scores